{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc86acd",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78b7f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MURA\n",
    "import cv2\n",
    "import image_manipulation\n",
    "from multiprocessing import Pool\n",
    "from models import *\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing\n",
    "import json\n",
    "from os import path, mkdir\n",
    "\n",
    "import keras\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22e578b7",
   "metadata": {},
   "source": [
    "# Model to Run\n",
    "This section lets the user edit on what model parameters to run, the model directory and parameters should exist prior to changing the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd350f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All editable variables\n",
    "model_file_path = \"models/model_1\"\n",
    "# If preprocessing needs to run\n",
    "run_preprocessing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b73251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model parameters\n",
    "try:\n",
    "    params = json.load(open(model_file_path + '/parameters.json'))\n",
    "\n",
    "    # Model to use\n",
    "    model_is_VAE = params['is_VAE']\n",
    "    # Model parameters\n",
    "    multiplier = params['multiplier']\n",
    "    latent_size = params['latent_size']\n",
    "    input_shape = params['input_shape']\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "\n",
    "    # Dataset Path\n",
    "    image_paths = MURA.MURA_DATASET()\n",
    "    dataset_file_path = params['dataset_path']\n",
    "    all_image_paths = image_paths.get_combined_image_paths()\n",
    "    all_image_paths = all_image_paths.to_numpy()[:,0]\n",
    "except:\n",
    "    raise Exception(\"No parameters.json file found in the model's directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd25378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directories...\n",
      "Shuffling Data...\n",
      "Processing and Augmenting images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:22<00:00, 70.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving files to correct directory\n",
      "Finished preprocessing data\n"
     ]
    }
   ],
   "source": [
    "# Do preprocessing\n",
    "if run_preprocessing:\n",
    "    preprocess = preprocessing.preprocessing(input_path = all_image_paths, output_path = dataset_file_path)\n",
    "    if __name__ == '__main__':\n",
    "        preprocess.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae204155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each array contains the training, validation, and testing in order\n",
    "image_datasets = {'train': [],\n",
    "                'valid': [],\n",
    "                'test': []}\n",
    "for dataset_name in image_datasets.keys():\n",
    "    for image_path in glob.glob(f'{dataset_file_path}/{dataset_name}/*.png'):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image_datasets[dataset_name].append(image)\n",
    "    image_datasets[dataset_name] = np.array(image_datasets[dataset_name])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69e69d0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This section creates and trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f906dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE ACCEPTED:  (None, 64, 64, 1)\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_decoder (encoder_de  multiple                 19946433  \n",
      " coder)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,946,439\n",
      "Trainable params: 19,935,937\n",
      "Non-trainable params: 10,502\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "Vanilla Loss\n",
      "Data Shape:  (None, 64, 64)\n",
      "Reconstructed Shape:  (None, 64, 64)\n",
      "Vanilla Loss\n",
      "Data Shape:  (None, 64, 64)\n",
      "Reconstructed Shape:  (None, 64, 64)\n",
      "133/133 [==============================] - ETA: 0s - mse_loss: 2283.7317 - reconstruction_loss: -902.5644 - kl_loss: 0.0000e+004\n",
      "callback predict\n",
      "133/133 [==============================] - 91s 665ms/step - mse_loss: 2283.7317 - reconstruction_loss: -902.5644 - kl_loss: 0.0000e+00 - val_mse_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "133/133 [==============================] - ETA: 0s - mse_loss: 1095.3171 - reconstruction_loss: -919.3143 - kl_loss: 0.0000e+004\n",
      "callback predict\n",
      "133/133 [==============================] - 85s 638ms/step - mse_loss: 1095.3171 - reconstruction_loss: -919.3143 - kl_loss: 0.0000e+00 - val_mse_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "133/133 [==============================] - ETA: 0s - mse_loss: 899.5062 - reconstruction_loss: -919.5865 - kl_loss: 0.0000e+004\n",
      "callback predict\n",
      "133/133 [==============================] - 89s 667ms/step - mse_loss: 899.5062 - reconstruction_loss: -919.5865 - kl_loss: 0.0000e+00 - val_mse_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if model_is_VAE:\n",
    "        model = VAE(False, input_shape, multiplier, latent_size)\n",
    "    else:\n",
    "        model = UPAE(True, input_shape, multiplier, latent_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.build(input_shape=(None,) + tuple(input_shape))\n",
    "\n",
    "    model.compile(optimizer= optimizer\n",
    "                  ,metrics=[tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "    model.summary()\n",
    "    # Where images of each epoch will be saved\n",
    "    # save_directory = 'Images/images_epochs' #edited in models where automatically make folder if non existent\n",
    "    save_callback = SaveImageCallback(image_datasets['train'])\n",
    "\n",
    "    # plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "    #training on training set.\n",
    "    history_train = model.fit(image_datasets['train'], \n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.15,\n",
    "                callbacks=[save_callback])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96a1d3c",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "353efa2a",
   "metadata": {},
   "source": [
    "# Plots Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b73f60ee",
   "metadata": {},
   "source": [
    "# Testing of the Model with the Test Set (CARA VERSION)\n",
    "This section tests the model with the current test set\n",
    "TODO: \n",
    "- Get the label of each image in the test set\n",
    "- Test the images\n",
    "- Create Linear Regression for the abnormality score to get the threshold for determining abnormal or normal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d082aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "labels = []\n",
    "for image_path in glob.glob(f'{dataset_file_path}/test/*.png'):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Get if it contains positive or negative\n",
    "    if 'positive' in image_path:\n",
    "        test_images.append(image)\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        test_images.append(image)\n",
    "        labels.append(0)\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7071dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not callback predict\n"
     ]
    }
   ],
   "source": [
    "history_valid = model.predict(image_datasets['test'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a2533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnor_scores = history_valid[1]\n",
    "#converting to an array of numbers instead of tensor\n",
    "abnor_scores = [item.numpy() for item in abnor_scores if isinstance(item, tf.Tensor)]\n",
    "abnor_scores = [float(item) for item in abnor_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3f048b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7488 7488\n"
     ]
    }
   ],
   "source": [
    "#checking if equal number of images\n",
    "print(len(abnor_scores),len(labels) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea8ade15",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(labels, abnor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb0fe041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting roc_curve output\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels, abnor_scores)\n",
    "idx = None\n",
    "\n",
    "#Computation for the threshold\n",
    "for i in range(len(fpr)):\n",
    "    fnr = 1 - tpr[i]\n",
    "    if abs(fpr[i] - fnr) <= 5e-3:\n",
    "        idx = i\n",
    "        break\n",
    "assert idx is not None\n",
    "t = thresholds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34fe4fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate:0.5482233502538071\n",
      "Precision:0.7536355859709153 Sensitivity:0.44705683355886333 Specificity:0.4517766497461929 f1:0.5612060728315108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using threshold t , will be used to classify abnormality scores as normal or abnormal in the y_pred array;\n",
    "y_pred = np.zeros_like(labels)\n",
    "y_pred[abnor_scores < t] = 0\n",
    "y_pred[abnor_scores >= t] = 1\n",
    "\n",
    "\n",
    "# getting metrics score using y_pred which is now either 0  or 1\n",
    "pres = metrics.precision_score(labels, y_pred)\n",
    "sens = metrics.recall_score(labels, y_pred, pos_label=1)\n",
    "spec = metrics.recall_score(labels, y_pred, pos_label=0)\n",
    "f1 = metrics.f1_score(labels, y_pred)\n",
    "print('Error rate:{}'.format(fpr[idx]))\n",
    "print('Precision:{} Sensitivity:{} Specificity:{} f1:{}\\n'.format(\n",
    "     pres, sens, spec, f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f5369b0",
   "metadata": {},
   "source": [
    "# Saving of final reconstructed images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154822a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory in models folder for reconstructed images\n",
    "dataset_name = dataset_file_path.split('/')[-1]\n",
    "reconstructed_images_path = model_file_path + \"/\" + dataset_name\n",
    "if not path.exists(reconstructed_images_path):\n",
    "    mkdir(reconstructed_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7d13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(history_valid)):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "    axs[0].imshow(image_datasets['test'][x])\n",
    "    axs[0].set_title('Original Image')\n",
    "    new_image = np.floor(history_valid[0][x]).astype(np.uint8)\n",
    "    axs[1].imshow(new_image)\n",
    "    axs[1].set_title('Reconstructed Image')\n",
    "    plt.savefig(f'{reconstructed_images_path}/Valid_Image_{x}.png')\n",
    "    plt.close()\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b430df",
   "metadata": {},
   "source": [
    "# Saving of Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324861d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_file_path + '/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_valid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46067ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = np.concatenate(history_valid[0], axis=1)\n",
    "plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_valid[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90fc7ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
